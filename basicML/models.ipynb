{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zjtsetThgNGF"
   },
   "source": [
    "# Deep Learning Models etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DlPofFf4gNGH"
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import import_ipynb\n",
    "import random\n",
    "import utils\n",
    "utils.hide_toggle('Imports 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zyWPOfx4gNGI"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from IPython import display\n",
    "utils.hide_toggle('Imports 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bf-ObTVjgNGI"
   },
   "source": [
    "Compute accuracy of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U1s2K8t9gNGI"
   },
   "outputs": [],
   "source": [
    "def accuracy(Net,X_test,y_test,verbose=True):\n",
    "    Net.eval()\n",
    "    m = X_test.shape[0]\n",
    "    y_pred = Net(X_test)\n",
    "    predicted = torch.max(y_pred, 1)[1]\n",
    "    correct = (predicted == y_test).float().sum().item()\n",
    "    if verbose: print(correct,m)\n",
    "    accuracy = correct/m\n",
    "    Net.train()\n",
    "    return accuracy\n",
    "utils.hide_toggle('Function: accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F-soed1XgPvk"
   },
   "outputs": [],
   "source": [
    "def accuracy_variable(Net,data):\n",
    "    step=0\n",
    "    acc=0\n",
    "    for (X,y) in data:\n",
    "            y_pred = Net(X)\n",
    "            step+=1\n",
    "            acc+=accuracy(Net,X,y,verbose=False)\n",
    "    a = acc/step\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kAO_qzaXgNGI"
   },
   "source": [
    "Generic training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DkOM1grhgNGJ"
   },
   "outputs": [],
   "source": [
    "def Train(Net,data,epochs=20,lr=5e-2,Loss=nn.NLLLoss(),verbose=False):\n",
    "    ### INSERT YOUR CODE HERE\n",
    "    optimizer = Net.optimizer\n",
    "    losses = []\n",
    "    accs = []\n",
    "    for epoch in range(epochs):\n",
    "      correct = 0\n",
    "      loss_ = 0.0\n",
    "      length = 0\n",
    "      for X_batch, y_batch in data:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = Net(X_batch)\n",
    "        pred = torch.max(predictions, 1)[1]\n",
    "        loss = Loss(predictions, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        correct += (pred == y_batch).float().sum().item()\n",
    "        loss_ += loss.item()\n",
    "        length += X_batch.shape[0]\n",
    "      acc = correct / length\n",
    "      accs.append(acc)\n",
    "      losses.append(loss.item())\n",
    "      print('Epoch [{}/{}], Loss: {:.7f}, Accuracy: {:.4f}'.format(epoch+1, epochs, loss_/length, acc))\n",
    "\n",
    "    return Net,losses,accs\n",
    "utils.hide_toggle('Function Train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7hmr1AlgNGJ"
   },
   "source": [
    "Multi-layer perceptron with ReLU non-lineartities; for classification or regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,dims=[5,3,2],task='classification',lr=1e-3):\n",
    "        super(MLP,self).__init__()\n",
    "        self.dims=dims\n",
    "        self.n = len(self.dims)-1\n",
    "        self.task=task\n",
    "        self.layers=nn.ModuleList()\n",
    "        for i in range(self.n-1):\n",
    "            self.layers.append(nn.Linear(dims[i],dims[i+1]))\n",
    "            self.layers.append(nn.ReLU())\n",
    "        if task=='classification': \n",
    "            self.layers.append(nn.Linear(dims[i+1],dims[i+2]))\n",
    "            self.layers.append(nn.LogSoftmax(dim=1))\n",
    "        elif task=='regression': \n",
    "            self.layers.append(nn.Linear(dims[i+1],dims[i+2]))\n",
    "            self.layers.append(nn.Linear(dims[i+2],1))\n",
    "        else: self.layers.append(nn.Linear(dims[i+1],dims[i+2]))\n",
    "        self.optimizer = optim.Adam(self.parameters(),lr=lr)\n",
    "    def forward(self,x):\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OH7lFdnrgNGK"
   },
   "source": [
    "Recurrent network using GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ROcNAY9igNGL"
   },
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size,lr):\n",
    "        # This just calls the base class constructor\n",
    "        super().__init__()\n",
    "        # Neural network layers assigned as attributes of a Module subclass\n",
    "        # have their parameters registered for training automatically.\n",
    "        self.input_size=input_size\n",
    "        self.rnn = torch.nn.RNN(input_size, hidden_size, nonlinearity='relu', batch_first=True)\n",
    "        self.linear = torch.nn.Linear(hidden_size, output_size)\n",
    "        self.logsoft = nn.LogSoftmax(dim=-1)\n",
    "        self.optimizer = optim.Adam(self.parameters(),lr=lr)\n",
    "    def forward(self, x):\n",
    "        # torch.nn.RNN also returns its hidden state but we donâ€™t use it.\n",
    "        # While the RNN can also take a hidden state as input, the RNN\n",
    "        # gets passed a hidden state initialized with zeros by default.\n",
    "        if self.input_size==1: x=x.unsqueeze(-1)\n",
    "        ### INSERT YOUR CODE HERE\n",
    "        hidden = torch.zeros(1, x.size(0), self.rnn.hidden_size)\n",
    "        x, _ = self.rnn(x, hidden)\n",
    "        x = self.linear(x)\n",
    "        x = self.logsoft(x)\n",
    "        x=x[:,-1,:]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "stMcIkq0gNGL"
   },
   "outputs": [],
   "source": [
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, lr):\n",
    "        super().__init__()\n",
    "        ### INSERT YOUR CODE HERE\n",
    "        self.input_size=input_size\n",
    "        self.hidden_size=hidden_size \n",
    "        self.lstm = torch.nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.linear = torch.nn.Linear(hidden_size, output_size)\n",
    "        self.logsoft = nn.LogSoftmax(dim=-1)\n",
    "        self.optimizer = optim.Adam(self.parameters(),lr=lr)\n",
    "    def forward(self, x):\n",
    "        if self.input_size==1: x=x.unsqueeze(-1)\n",
    "        #### INSERT YOUR CODE HERE\n",
    "        hidden = torch.zeros(1, x.size(0), self.hidden_size)\n",
    "        cell = torch.zeros(1, x.size(0), self.hidden_size)\n",
    "        x, _ = self.lstm(x, (hidden, cell))\n",
    "        x = self.linear(x)\n",
    "        x = self.logsoft(x)\n",
    "        x=x[:,-1,:]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F1VNWZLepNlc"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class OneDimCNN(nn.Module):\n",
    "    def __init__(self,input_size, hidden_size, output_size, lr):\n",
    "        super().__init__()\n",
    "        self.input_size=input_size\n",
    "        self.hidden_size=hidden_size\n",
    "        self.conv_1 = nn.Conv1d(in_channels=input_size, out_channels=hidden_size, kernel_size=5, stride=2)\n",
    "        self.conv_2 = nn.Conv1d(in_channels=hidden_size, out_channels=output_size, kernel_size=1)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(25)\n",
    "        self.linear = nn.Linear(25*output_size, output_size)\n",
    "        self.logsoft = nn.LogSoftmax(dim=-1)\n",
    "        self.optimizer = optim.Adam(self.parameters(),lr=lr)\n",
    "    def forward(self, x):\n",
    "        if self.input_size==1: x=x.unsqueeze(-1)\n",
    "        x = x.transpose(1,2)\n",
    "        x = self.conv_1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv_2(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.flatten(1)\n",
    "        x = self.linear(x)\n",
    "        x = self.logsoft(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D8GwP-IPvbk5"
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size,lr):\n",
    "        # This just calls the base class constructor\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.d_model = 128\n",
    "        self.linear_1 = nn.Linear(self.input_size, self.d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=self.d_model, nhead=8, dim_feedforward=hidden_size)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear_2 = nn.Linear(self.d_model, output_size)\n",
    "        self.logsoft = nn.LogSoftmax(dim=-1)\n",
    "        self.optimizer = optim.Adam(self.parameters(),lr=lr)\n",
    "    def forward(self, x):\n",
    "        if self.input_size==1: x=x.unsqueeze(-1)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.linear_1(x)\n",
    "        x = self.relu(x)\n",
    "        #Positional Encoding\n",
    "        max_len = 25\n",
    "        position = torch.arange(25).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.d_model, 2) * (-np.math.log(10000.0) / self.d_model))\n",
    "        pe = torch.zeros(max_len, 1, self.d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        x = x + pe[:x.size(0)]\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.transpose(1, 0)\n",
    "        x = x.max(1)[0]\n",
    "        x = self.relu(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = self.logsoft(x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "models.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
